{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark MLlib\n",
    "\n",
    "```{note}\n",
    "我们从 Airbnb 房屋租金预测这个小项目入手，来学习 Spark MLlib 的基本用法。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"design ml\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|neighbourhood_cleansed|      room_type|bedrooms|bathrooms|number_of_reviews|price|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "|      Western Addition|Entire home/apt|     1.0|      1.0|            180.0|170.0|\n",
      "|        Bernal Heights|Entire home/apt|     2.0|      1.0|            111.0|235.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|             17.0| 65.0|\n",
      "|        Haight Ashbury|   Private room|     1.0|      4.0|              8.0| 65.0|\n",
      "|      Western Addition|Entire home/apt|     2.0|      1.5|             27.0|785.0|\n",
      "+----------------------+---------------+--------+---------+-----------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filePath = \"../data/sf-airbnb/sf-airbnb-clean.parquet/\"\n",
    "airbnbDF = spark.read.parquet(filePath)\n",
    "airbnbDF.select(\"neighbourhood_cleansed\", \"room_type\", \"bedrooms\", \"bathrooms\",\n",
    "                \"number_of_reviews\", \"price\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练和测试集\n",
    "trainDF, testDF = airbnbDF.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程\n",
    "\n",
    "`VectorAssembler` takes a list of input columns and creates a new DataFrame with an additional column which combines\n",
    "the values of those input columns into a single vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+\n",
      "|bedrooms|features|price|\n",
      "+--------+--------+-----+\n",
      "|     1.0|   [1.0]|200.0|\n",
      "|     1.0|   [1.0]|130.0|\n",
      "|     1.0|   [1.0]| 95.0|\n",
      "|     1.0|   [1.0]|250.0|\n",
      "|     3.0|   [3.0]|250.0|\n",
      "|     1.0|   [1.0]|115.0|\n",
      "|     1.0|   [1.0]|105.0|\n",
      "|     1.0|   [1.0]| 86.0|\n",
      "|     1.0|   [1.0]|100.0|\n",
      "|     2.0|   [2.0]|220.0|\n",
      "+--------+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"bedrooms\"], \n",
    "                               outputCol=\"features\")\n",
    "vecTrainDF = vecAssembler.transform(trainDF)\n",
    "vecTrainDF.select(\"bedrooms\", \"features\", \"price\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建和训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"price\")\n",
    "lrModel = lr.fit(vecTrainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The formula for the linear regression line is price = 123.68*bedrooms + 47.51\n"
     ]
    }
   ],
   "source": [
    "m = round(lrModel.coefficients[0], 2)\n",
    "b = round(lrModel.intercept, 2)\n",
    "print(f\"The formula for the linear regression line is price = {m}*bedrooms + {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 Pipeline\n",
    "\n",
    "便于复用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+------+------------------+\n",
      "|bedrooms|features| price|        prediction|\n",
      "+--------+--------+------+------------------+\n",
      "|     1.0|   [1.0]|  85.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  45.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  70.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 128.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 159.0|171.18598011578285|\n",
      "|     2.0|   [2.0]| 250.0|294.86172649777757|\n",
      "|     1.0|   [1.0]|  99.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|  95.0|171.18598011578285|\n",
      "|     1.0|   [1.0]| 100.0|171.18598011578285|\n",
      "|     1.0|   [1.0]|2010.0|171.18598011578285|\n",
      "+--------+--------+------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preDF = pipelineModel.transform(testDF)\n",
    "preDF.select(\"bedrooms\", \"features\", \"price\", \"prediction\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot 编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "categoricalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                   if dataType == \"string\"]\n",
    "indexOutputCols = [x + \"Index\" for x in categoricalCols]\n",
    "oheOutputCols = [x + \"OHE\" for x in categoricalCols]\n",
    "\n",
    "# 首先使用 StringIndexer 把字符串转化为 index\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, \n",
    "                              outputCols=indexOutputCols, \n",
    "                              handleInvalid=\"skip\")\n",
    "# 再使用 OneHotEncoder 把 index 转化为 one-hot\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                           outputCols=oheOutputCols)\n",
    "\n",
    "numericalCols = [field for (field, dataType) in trainDF.dtypes \n",
    "                 if ((dataType == \"double\") & (field != \"price\"))]\n",
    "vecAssembler = VectorAssembler(inputCols=oheOutputCols + numericalCols,\n",
    "                               outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|price|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(98,[0,3,6,22,43,...| 85.0| 55.24365707389188|\n",
      "|(98,[0,3,6,22,43,...| 45.0|23.357685914717877|\n",
      "|(98,[0,3,6,22,43,...| 70.0|28.474464479034395|\n",
      "|(98,[0,3,6,12,42,...|128.0| -91.6079079594947|\n",
      "|(98,[0,3,6,12,43,...|159.0| 95.05688229945372|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(labelCol=\"price\", featuresCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[stringIndexer, oheEncoder, vecAssembler, lr])\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "predDF.select(\"features\", \"price\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 220.6\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol=\"prediction\", \n",
    "                                          labelCol=\"price\", \n",
    "                                          metricName=\"rmse\")\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another metric:\n",
    "\n",
    "$$R^{2} = 1 - \\frac{SS_{res}}{SS_{tot}}$$\n",
    "\n",
    "where $SS_{tot}$ is the total sum of square if you always predict $\\bar{y}$:\n",
    "\n",
    "$$SS_{tot} = \\sum_{i=1}^{n}(y_{i} - \\bar{y})^{2}$$\n",
    "\n",
    "and $SS_{res}$ is the sum of residuals squared from your model predictions:\n",
    "\n",
    "$$SS_{res} = \\sum_{i=1}^{n}(y_{i} - \\hat{y}_{i})^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.16043316698848087\n"
     ]
    }
   ],
   "source": [
    "r2 = regressionEvaluator.setMetricName(\"r2\").evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存和加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存\n",
    "pipelinePath = \"../data/tmp/lr-pipeline-model\"\n",
    "pipelineModel.write().overwrite().save(pipelinePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "# 加载\n",
    "savedPipelineModel = PipelineModel.load(pipelinePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 树模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor_bb58b45f8dac"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(labelCol=\"price\")\n",
    "\n",
    "# 在 DecisionTree 中不使用 one-hot 编码\n",
    "vecAssembler = VectorAssembler(inputCols=indexOutputCols + numericalCols, \n",
    "                               outputCol=\"features\")\n",
    "# crucial for performing distributed training\n",
    "# requires maxBins >= the number of values in each categorical feature\n",
    "dt.setMaxBins(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = [stringIndexer, vecAssembler, dt]\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel: uid=DecisionTreeRegressor_bb58b45f8dac, depth=5, numNodes=47, numFeatures=33\n",
      "  If (feature 12 <= 2.5)\n",
      "   If (feature 12 <= 1.5)\n",
      "    If (feature 5 in {1.0,2.0})\n",
      "     If (feature 4 in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 104.23992784125075\n",
      "      Else (feature 3 not in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,23.0,24.0,25.0,26.0,27.0,28.0,29.0,30.0,31.0,32.0,33.0,34.0})\n",
      "       Predict: 250.7111111111111\n",
      "     Else (feature 4 not in {0.0,1.0,3.0,5.0,9.0,10.0,11.0,13.0,14.0,16.0,18.0,24.0})\n",
      "      If (feature 3 in {0.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0,11.0,12.0,13.0,14.0,15.0,16.0,17.0,18.0,19.0,20.0,21.0,22.0,23.0,27.0,33.0,35.0})\n",
      "       Predict: 151.94179894179894\n",
      "      Else (feat\n"
     ]
    }
   ],
   "source": [
    "# extract the if-then-else rules learned by the decision tree\n",
    "dtModel = pipelineModel.stages[-1]\n",
    "print(dtModel.toDebugString[: 1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>bedrooms</td>\n",
       "      <td>0.283406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cancellation_policyIndex</td>\n",
       "      <td>0.167893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>instant_bookableIndex</td>\n",
       "      <td>0.140081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>property_typeIndex</td>\n",
       "      <td>0.128179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>number_of_reviews</td>\n",
       "      <td>0.126233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neighbourhood_cleansedIndex</td>\n",
       "      <td>0.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>longitude</td>\n",
       "      <td>0.038810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>minimum_nights</td>\n",
       "      <td>0.029473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>beds</td>\n",
       "      <td>0.015218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>room_typeIndex</td>\n",
       "      <td>0.010905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        feature  importance\n",
       "12                     bedrooms    0.283406\n",
       "1      cancellation_policyIndex    0.167893\n",
       "2         instant_bookableIndex    0.140081\n",
       "4            property_typeIndex    0.128179\n",
       "15            number_of_reviews    0.126233\n",
       "3   neighbourhood_cleansedIndex    0.056200\n",
       "9                     longitude    0.038810\n",
       "14               minimum_nights    0.029473\n",
       "13                         beds    0.015218\n",
       "5                room_typeIndex    0.010905"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# extract the feature importance scores\n",
    "featureImp = pd.DataFrame(list(zip(vecAssembler.getInputCols(), \n",
    "                                   dtModel.featureImportances)), \n",
    "                          columns=[\"feature\", \"importance\"])\n",
    "featureImp.sort_values(by=\"importance\", ascending=False)[: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机森林"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(labelCol=\"price\", maxBins=40, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# 创建随机森林 pipeline\n",
    "pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])\n",
    "# 搜索空间\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [2, 4, 6])\n",
    "             .addGrid(rf.numTrees, [10, 100])\n",
    "             .build())\n",
    "evaluator = RegressionEvaluator(labelCol=\"price\",\n",
    "                                predictionCol=\"prediction\",\n",
    "                                metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \n",
    "                    evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, \n",
    "                    numFolds=3, seed=42)\n",
    "cvModel = cv.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({Param(parent='RandomForestRegressor_7881b42f1736', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_7881b42f1736', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  291.18226409247836),\n",
       " ({Param(parent='RandomForestRegressor_7881b42f1736', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 2,\n",
       "   Param(parent='RandomForestRegressor_7881b42f1736', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  286.7714750274078),\n",
       " ({Param(parent='RandomForestRegressor_7881b42f1736', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_7881b42f1736', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  287.6963245160818),\n",
       " ({Param(parent='RandomForestRegressor_7881b42f1736', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 4,\n",
       "   Param(parent='RandomForestRegressor_7881b42f1736', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  279.99270572360797),\n",
       " ({Param(parent='RandomForestRegressor_7881b42f1736', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_7881b42f1736', name='numTrees', doc='Number of trees to train (>= 1).'): 10},\n",
       "  294.34810870889305),\n",
       " ({Param(parent='RandomForestRegressor_7881b42f1736', name='maxDepth', doc='Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30].'): 6,\n",
       "   Param(parent='RandomForestRegressor_7881b42f1736', name='numTrees', doc='Number of trees to train (>= 1).'): 100},\n",
       "  275.3986270472998)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取验证结果\n",
    "list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化 Pipeline\n",
    "\n",
    "Even though each of the models in the cross validator is technically independent, spark.ml actually trains the collection of models sequentially rather than in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加速训练，并行度为4\n",
    "cvModel = cv.setParallelism(4).fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CrossValidator(estimator=rf,\n",
    "                    evaluator=evaluator,\n",
    "                    estimatorParamMaps=paramGrid,\n",
    "                    numFolds=3,\n",
    "                    parallelism=4,\n",
    "                    seed=42)\n",
    "\n",
    "# won’t be reevaluating the StringIndexer\n",
    "# 速度更快一些\n",
    "pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])\n",
    "pipelineModel = pipeline.fit(trainDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
