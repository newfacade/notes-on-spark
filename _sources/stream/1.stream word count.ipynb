{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 流动的 Word Count\n",
    "\n",
    "```{note}\n",
    "今天的大数据处理，对于延迟性的要求越来越高，因此流处理的基本概念与工作原理，是每一个大数据从业者必备的“技能点”。<br/>\n",
    "我们从一个”流动的 Word Count“入手，去学习一下在流计算的框架下，Word Count 是怎么做的。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结构\n",
    "\n",
    "在流计算场景中，Source 是流计算的数据源头；流处理引擎在数据流动过程中实现数据处理，保证数据完整性与一致性；Sink 指的是数据流向的目的地。\n",
    "\n",
    "![jupyter](../images/stream.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "在“流动的 Word Count”里，数据以行为粒度，分批地“喂给”Spark，每一行数据，都会触发一次 Job 计算。\n",
    "\n",
    "具体来说，我们使用 netcat 工具，向本地 9999 端口的 Socket 地址发送数据行：\n",
    "\n",
    "![jupyter](../images/netcat.jpg)\n",
    "\n",
    "而 Spark 流处理应用，则时刻监听着本机的 9999 端口，一旦接收到数据条目，就会立即触发计算逻辑的执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 流处理引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"stream word count\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从监听地址创建 DataFrame\n",
    "df = (spark.readStream\n",
    "      .format(\"socket\")\n",
    "      .option(\"host\", \"127.0.0.1\")\n",
    "      .option(\"port\", 9999)\n",
    "      .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# 先把字符串以空格为分隔符做拆分，得到单词数组 words\n",
    "# 再把数组 words 展平为单词 word\n",
    "df = (df\n",
    "      .withColumn(\"words\", F.split(\"value\", \" \"))\n",
    "      .withColumn(\"word\", F.explode(\"words\"))\n",
    "      .groupBy(\"word\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sink\n",
    "\n",
    "在 Complete mode 下，每一批次的计算结果，都会包含系统到目前为止处理的全部数据内容。<br/>\n",
    "在 Update mode 下，每个批次仅输出内容有变化的数据记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定Sink为终端\n",
    "# 指定输出选项\n",
    "# 指定输出模式\n",
    "# 启动流处理应用\n",
    "# 等待中断指令\n",
    "(df.writeStream.format(\"console\")\n",
    " .option(\"truncate\", False)\n",
    " .outputMode(\"complete\")\n",
    " .start()\n",
    " .awaitTermination())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
