{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch mode 和 Continuous mode\n",
    "\n",
    "```{note}\n",
    "Structured Streaming 有 Batch mode 和 Continuous mode 这两种计算模型和 4 种 Trigger 机制。<br/>\n",
    "Batch mode 吞吐量大、延迟高（秒级）。<br/>\n",
    "Continuous mode 吞吐量低、延迟也更低（毫秒级）。\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trgger 机制\n",
    "\n",
    "当数据像水流一样，源源不断地流进 Structured Streaming 引擎的时候，引擎并不会自动地依次消费并处理这些数据，它需要一种叫做 Trigger 的机制，来触发数据在引擎中的计算。\n",
    "\n",
    "Structured Streaming 支持 4 种 Trigger，它们所基于的计算模型和含义如下：\n",
    "\n",
    "![jupyter](../images/trigger.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch mode\n",
    "\n",
    "所谓 Batch mode，它指的是 Spark 将连续的数据流，切割为离散的数据微批进行处理。\n",
    "\n",
    "形象一点说，Batch mode 就像是“抽刀断水”，两刀之间的水量，就是一个 Micro-batch。而每一份 Micro-batch，都会触发一个 Spark Job，最终交由 Spark SQL 与 Spark Core 去做优化与执行。\n",
    "\n",
    "![jupyter](../images/batch.webp)\n",
    "\n",
    "在 Default Trigger 下，Spark 会根据数据流的流入速率，自行决定切割粒度，无需开发者关心。<br/>\n",
    "Fixed interval Trigger 明确定义了 Micro-batch 切割的时间周期。<br/>\n",
    "而 One-time Trigger 会一次性处理所有数据流。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous mode\n",
    "\n",
    "Continuous mode 以事件/消息为粒度，用连续的方式来处理数据。这里的事件或消息，指的是原始数据流中最细粒度的数据形式，它可以是一个单词、一行文本，或是一个画面帧。\n",
    "\n",
    "在 Continuous mode 下，Structured Streaming 使用一个常驻作业来处理数据流（或者说服务）中的每一条消息。\n",
    "\n",
    "![jupyter](../images/continuous.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 容错机制\n",
    "\n",
    "所谓容错，它指的是，在计算过程中出现错误的时候，流处理引擎有能力恢复被中断的计算过程，同时保证数据上的不重不漏。\n",
    "\n",
    "Batch mode 利用 Checkpoint 机制来实现容错。在实际处理数据流中的 Micro-batch **之前**，Checkpoint 机制会把该 Micro-batch 的元信息全部存储到开发者指定的文件系统路径。这样一来，当出现作业或是任务失败时，引擎只需要读取这些事先记录好的元信息，就可以恢复被中段的计算。\n",
    "\n",
    "```python\n",
    "# 指定 Checkpoint 存储地址的代码示例\n",
    ".option(\"checkpointLocation\", \"path/to/HDFS\")\n",
    "```\n",
    "\n",
    "Spark 为 Continuous mode 下的容错引入了 Epoch Marker 机制：\n",
    "\n",
    "```python\n",
    "writeStream.trigger(continuous = \"1 second\")\n",
    "```\n",
    "\n",
    "上面的代码中，Structured Streaming 每隔 1 秒，就会安插一个 Epoch Marker，而两个 Epoch Marker 之间的数据，就称为一个 Epoch。<br/>\n",
    "在引擎处理并交付数据的过程中，每当遇到 Epoch Marker 的时候，引擎都会把对应 Epoch 中**最后一条**消息的 Offset 写入日志，从而实现容错。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
