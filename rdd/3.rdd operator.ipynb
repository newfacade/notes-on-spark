{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD 常用算子\n",
    "\n",
    "```{note}\n",
    "从是否触发计算来说，RDD 算子可分为 Transformations 类算子和 Actions 类算子。<br/>\n",
    "从算子用途来说，RDD 有数据转换、数据聚合、数据持久化等类型的算子。\n",
    "```\n",
    "\n",
    "| 算子类型        | 适用范围   | 算子用途      | 算子集合                                        |\n",
    "|:-----------------|:------------|:---------------|:-------------------------------------------------|\n",
    "| Transformations | 任意RDD    | RDD内数据转换 | map<br/>mapPartitions<br/>flatMap<br/>filter                |\n",
    "|                 | Paired RDD | RDD内数据聚合 | groupByKey<br/>reduceByKey<br/>aggregateByKey<br/>sortByKey<br/>sortBy |\n",
    "|                 | 任意RDD    | 数据整合      | union<br/>sample                                    |\n",
    "|                 | 任意RDD    | 重分布        | coalesce<br/>repartition                            |\n",
    "| Actions         | 任意RDD    | 数据收集      | collect<br/>first<br/>take                              |\n",
    "|                 | 任意RDD    | 数据持久化    | saveAsTextFile                                  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD内数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"rdd operator\").setMaster(\"local\")\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "# 第一节的代码\n",
    "lineRDD = sc.textFile(\"../data/wikiOfSpark.txt\")\n",
    "# flatMap: 先从元素到集合、再从集合到元素\n",
    "wordRDD = lineRDD.flatMap(lambda line: line.split(\" \"))\n",
    "# 调用 filter(f)，其作用是保留 RDD 中 f 返回 True 的数据元素，过滤其它元素。\n",
    "cleanWordRDD = wordRDD.filter(lambda word: word != \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "\n",
    "给定映射函数 f，map(f) 以元素为粒度对 RDD 做数据转换。<br/>\n",
    "f 可以是带名函数也可以是匿名函数，效果都是一样的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(word):\n",
    "    return (word, 1)\n",
    "\n",
    "# 带名函数\n",
    "kvRDD = cleanWordRDD.map(f)\n",
    "# 匿名函数，效果同上\n",
    "kvRDD = cleanWordRDD.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mapPartitions\n",
    "\n",
    "以数据分区为粒度，使用映射函数 f 对 RDD 进行数据转换。<br/>\n",
    "可以使用 mapPartitions 来改善执行性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('e9713ae04a02a810d6f33dd956f42794', 1), ('032d2d5e07dd65f436bf59e8135822d2', 1)]\n"
     ]
    }
   ],
   "source": [
    "from hashlib import md5\n",
    "\n",
    "def f(partition):\n",
    "    \"\"\"在处理每一条数据记录的时候，可用复用同一个Partition内的md5对象\"\"\"\n",
    "    m = md5()\n",
    "    for word in partition:\n",
    "        m.update(word.encode())\n",
    "        yield m.hexdigest()\n",
    "        \n",
    "# 先加密 word，再做转化\n",
    "kvMD5RDD = cleanWordRDD.mapPartitions(f).map(lambda word: (word, 1))\n",
    "print(kvMD5RDD.take(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD内数据聚合\n",
    "\n",
    "接下来要介绍5个聚合算子，在它们计算的过程中，都会引入 Shuffle。\n",
    "\n",
    "### groupByKey\n",
    "\n",
    "groupByKey 的功能是对 Key 值相同的元素做分组，然后把相应的 Value 值，以`集合`的形式收集到一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key 和 Value 都变为单词\n",
    "kvSameRDD = cleanWordRDD.map(lambda word: (word, word))\n",
    "# [(Spark, (Spark, Spark, Spark)), (Streaming, (Streaming, Streaming))] 这样的数据\n",
    "words = kvSameRDD.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "\n",
    "分组聚合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 随机的value\n",
    "kvRandomRDD = cleanWordRDD.map(lambda word: (word, random.randint(0, 100)))\n",
    "# 聚合函数: 同组内最大的value\n",
    "wordCounts = kvRandomRDD.reduceByKey(lambda x, y: max(x, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregateByKey\n",
    "\n",
    "reduceByKey: Map 端聚合函数和 Reduce 端聚合函数都一样。<br/>\n",
    "aggregateByKey: 分别指定 Map 端聚合函数和 Reduce 端聚合函数。<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    # 定义 Map 阶段聚合函数\n",
    "    return x + y\n",
    "\n",
    "def f2(x, y):\n",
    "    # 定义 Reduce 阶段聚合函数\n",
    "    return max(x, y)\n",
    "\n",
    "# 初始值（需与f2结果类型保持一致），Map 函数，Reduce 函数\n",
    "wordCounts = kvRDD.aggregateByKey(0, f1, f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey 和 sortBy\n",
    "\n",
    "以 Key 为准对 RDD 做排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('|conference=', 2), ('|', 3), ('your', 1), ('you', 3), ('years', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(wordCounts.sortByKey(ascending=False).take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 67), ('Spark', 63), ('a', 54), ('and', 51), ('of', 50)]\n"
     ]
    }
   ],
   "source": [
    "print(wordCounts.sortBy(lambda x: x[1], ascending=False).take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据整合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1 = [\"Spark\", \"is\", \"cool\"]\n",
    "words2 = [\"what\", \"is\", \"Apache\"]\n",
    "rdd1 = sc.parallelize(words1)\n",
    "rdd2 = sc.parallelize(words2)\n",
    "\n",
    "# union: 合并两个同类型RDD\n",
    "rdd = rdd1.union(rdd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 7, 9, 13, 16, 39, 48, 53, 55, 71]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(list(range(100)))\n",
    "# sample: 对RDD做随机采样\n",
    "# 采样是否有放回，采样比例，随机数种子（可选）\n",
    "print(rdd.sample(False, 0.1, 123).take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "20\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 查看分区数量\n",
    "print(rdd.getNumPartitions())\n",
    "\n",
    "# repartition: 调整RDD的并行度（即RDD的数据分区数量），可增可降\n",
    "rdd1 = rdd.repartition(20)\n",
    "print(rdd1.getNumPartitions())\n",
    "\n",
    "# coalesce: 降低RDD的并行度，不会触发shuffle\n",
    "rdd2 = rdd1.coalesce(5)\n",
    "print(rdd2.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据收集\n",
    "\n",
    "take 我们已经很熟悉了，即收集数个元素。<br/>\n",
    "collect: 收集所有元素。<br/>\n",
    "first: 收集所有元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(rdd.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据持久化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"../data/persist\"\n",
    "if not os.path.exists(path=path):\n",
    "    cleanWordRDD.saveAsTextFile(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
